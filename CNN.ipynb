{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "这里我只用了1/3的data做测试，正式的时候记得改\n",
    "    1.文件名 \n",
    "    2.改x_train等的范围 \n",
    "    3.加入vaildation set\n",
    "    ps 个人感觉可以8：1：1\n",
    "'''\n",
    "\n",
    "#load the input data\n",
    "with open('data/test2.txt') as f:\n",
    "    sentence = f.readlines()\n",
    "x_train_raw = sentence[:901]\n",
    "x_test_raw = sentence[901:]\n",
    "\n",
    "#load the target data\n",
    "target = []\n",
    "with open('data/test3.txt') as f:\n",
    "    for i in f:\n",
    "        target.append(int(i))\n",
    "y_train = target[:901]\n",
    "y_test = target[901:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1812\n"
     ]
    }
   ],
   "source": [
    "#word embedding\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(set(sum([x.split() for x in sentence],[])))\n",
    "print(tok.document_count)\n",
    "\n",
    "#find the max sentence(we can set by ourselves if we know the number)\n",
    "max_len = max([len(x.split()) for x in sentence])\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "x_train = pad_sequences(tok.texts_to_sequences(x_train_raw), maxlen = max_len, padding = 'post')\n",
    "x_test = pad_sequences(tok.texts_to_sequences(x_test_raw), maxlen = max_len, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "900/900 [==============================] - 1s 978us/step - loss: 0.6938 - acc: 0.4644\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - 0s 211us/step - loss: 0.6720 - acc: 0.6122\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - 0s 147us/step - loss: 0.4618 - acc: 0.8422\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - 0s 138us/step - loss: 0.1944 - acc: 0.9411\n",
      "Epoch 5/10\n",
      "900/900 [==============================] - 0s 139us/step - loss: 0.0805 - acc: 0.9844\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - 0s 142us/step - loss: 0.0309 - acc: 0.9944\n",
      "Epoch 7/10\n",
      "900/900 [==============================] - 0s 170us/step - loss: 0.0142 - acc: 0.9967\n",
      "Epoch 8/10\n",
      "900/900 [==============================] - 0s 145us/step - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "900/900 [==============================] - 0s 137us/step - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "900/900 [==============================] - 0s 138us/step - loss: 0.0030 - acc: 1.0000\n",
      "900/900 [==============================] - 0s 225us/step\n",
      "Training Accuracy: 1.0000\n",
      "100/100 [==============================] - 0s 31us/step\n",
      "Testing Accuracy: 0.8400\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "conv1d(output size, filter size)\n",
    "结构：\n",
    "    word embedding: 因为这组dataset最多就31个单词，所以这里dimension只用10\n",
    "                    在正式做CNN前可以加一层dropout，也能有效反正overfitting\n",
    "    \n",
    "    1层convolution 每层结尾 maxpooling\n",
    "    Flatten  (用于maxpooling后连接到dense layer的转变)\n",
    "    1层dense\n",
    "    dropout  (.3 的掉率,用于防止overfitting.)\n",
    "    ouput    (用softmax)\n",
    "'''\n",
    "embedding_dim = 10\n",
    "model = Sequential()\n",
    "\n",
    "#word embedding\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length = max_len))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "#layer 1\n",
    "model.add(Conv1D(64, 5, activation = 'relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "          \n",
    "#layer 2\n",
    "# model.add(Conv1D(64, 5, activation = 'relu'))\n",
    "# model.add(MaxPooling1D(5))\n",
    "          \n",
    "#layer 3\n",
    "# model.add(Conv1D(64, 5, activation = 'relu'))\n",
    "# model.add(MaxPooling1D(5))\n",
    "          \n",
    "#flatten\n",
    "model.add(Flatten())\n",
    "          \n",
    "#dense\n",
    "model.add(Dense(64,activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "          \n",
    "#training and get score\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics = ['accuracy'])\n",
    "history = model.fit(x_train, y_train, batch_size = 10, epochs = 10)\n",
    "score = model.evaluate(x_train, y_train)\n",
    "print(\"Training Accuracy: {:.4f}\".format(score[1]))\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(\"Testing Accuracy: {:.4f}\".format(score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
